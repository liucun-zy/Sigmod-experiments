#!/usr/bin/env python3
"""
æ‰¹é‡é¢„å¤„ç†æ¨¡å—è¿è¡Œè„šæœ¬

æ”¯æŒï¼š
1. æ‰¹é‡å¤„ç†å¤šä¸ªJSONæ–‡ä»¶
2. ç›®å½•æ‰«æè‡ªåŠ¨å‘ç°æ–‡ä»¶
3. å¹¶è¡Œå¤„ç†ï¼ˆå¯é€‰ï¼‰
4. è¯¦ç»†çš„å¤„ç†æŠ¥å‘Š
5. é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶

ä½¿ç”¨è¯´æ˜ï¼š
1. ä¿®æ”¹ä¸‹é¢çš„é…ç½®å‚æ•°
2. è¿è¡Œè„šæœ¬ï¼špython run_batch_preprocess.py
"""

import os
import sys
import glob
import time
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ preprocess_moduleåˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from preprocess_module import BatchPreprocessPipeline, PreprocessConfig

# å¯¼å…¥é…ç½®
from batch_config import (
    INPUT_CONFIG, 
    PERFORMANCE_CONFIG, 
    ERROR_HANDLING_CONFIG,
    create_preprocess_config,
    get_config_for_environment,
    check_system_requirements,
    validate_config
)

# ==================== é…ç½®åŒºåŸŸ ====================

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–è®¾ç½®
INPUT_FILES = INPUT_CONFIG["specific_files"]
INPUT_DIRECTORY = INPUT_CONFIG["directory_scan"]["directory"] if INPUT_CONFIG["directory_scan"]["enabled"] else None
PATTERN = INPUT_CONFIG["directory_scan"]["pattern"]
OUTPUT_BASE_DIR = INPUT_CONFIG["output_base_dir"]
SUBFOLDER_PROCESSING = INPUT_CONFIG.get("subfolder_processing", {}).get("enabled", False)

# æ€§èƒ½é…ç½®
ENABLE_PARALLEL = PERFORMANCE_CONFIG["parallel_processing"]["enabled"]
MAX_WORKERS = PERFORMANCE_CONFIG["parallel_processing"]["max_workers"]

# é”™è¯¯å¤„ç†é…ç½®
CONTINUE_ON_ERROR = ERROR_HANDLING_CONFIG["continue_on_error"]
MAX_RETRIES = ERROR_HANDLING_CONFIG["max_retries"]

# å¤„ç†é…ç½®ï¼ˆå¯ä»¥é€‰æ‹©ç¯å¢ƒï¼šdevelopment, testing, productionï¼‰
ENVIRONMENT = "production"  # ä¿®æ”¹è¿™é‡Œé€‰æ‹©ä¸åŒçš„é…ç½®æ¨¡å¼
config = get_config_for_environment(ENVIRONMENT)

# ==================== ä¸»ç¨‹åº ====================

def discover_json_files(directory: str, pattern: str = "**/*.json") -> List[str]:
    """
    è‡ªåŠ¨å‘ç°ç›®å½•ä¸­çš„JSONæ–‡ä»¶
    
    Args:
        directory: æœç´¢ç›®å½•
        pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        
    Returns:
        JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if not directory or not os.path.exists(directory):
        return []
    
    search_path = Path(directory)
    json_files = []
    
    try:
        # ä½¿ç”¨globæ¨¡å¼æœç´¢æ–‡ä»¶
        for file_path in search_path.glob(pattern):
            if file_path.is_file() and file_path.suffix.lower() == '.json':
                json_files.append(str(file_path))
        
        print(f"ğŸ“ åœ¨ç›®å½• {directory} ä¸­å‘ç° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ æœç´¢ç›®å½•æ—¶å‡ºé”™: {e}")
    
    return json_files

def discover_subfolder_json_files(base_directory: str, exclude_patterns: List[str] = None) -> Dict[str, List[str]]:
    """
    æ‰«æå­æ–‡ä»¶å¤¹ä¸­çš„JSONæ–‡ä»¶
    
    Args:
        base_directory: åŸºç¡€ç›®å½•
        exclude_patterns: æ’é™¤çš„æ–‡ä»¶å¤¹æ¨¡å¼
        
    Returns:
        å­—å…¸ï¼Œé”®ä¸ºå­æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå€¼ä¸ºè¯¥æ–‡ä»¶å¤¹ä¸­çš„JSONæ–‡ä»¶åˆ—è¡¨
    """
    if not base_directory or not os.path.exists(base_directory):
        return {}
    
    if exclude_patterns is None:
        exclude_patterns = []
    
    base_path = Path(base_directory)
    subfolder_files = {}
    
    try:
        # éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for subfolder in base_path.iterdir():
            if not subfolder.is_dir():
                continue
                
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ’é™¤æ¨¡å¼
            exclude_folder = False
            for exclude_pattern in exclude_patterns:
                if subfolder.name.endswith(exclude_pattern.lstrip('*')):
                    exclude_folder = True
                    break
            
            if exclude_folder:
                print(f"â­ï¸ è·³è¿‡æ–‡ä»¶å¤¹: {subfolder.name}")
                continue
            
            # æœç´¢è¯¥æ–‡ä»¶å¤¹ä¸­çš„JSONæ–‡ä»¶
            json_files = []
            for json_file in subfolder.glob("*.json"):
                if json_file.is_file():
                    json_files.append(str(json_file))
            
            if json_files:
                subfolder_files[str(subfolder)] = json_files
                print(f"ğŸ“ æ–‡ä»¶å¤¹ {subfolder.name} ä¸­å‘ç° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            else:
                print(f"âš ï¸ æ–‡ä»¶å¤¹ {subfolder.name} ä¸­æœªå‘ç°JSONæ–‡ä»¶")
        
        total_files = sum(len(files) for files in subfolder_files.values())
        print(f"ğŸ“Š æ€»å…±å‘ç° {len(subfolder_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶å¤¹ï¼ŒåŒ…å« {total_files} ä¸ªJSONæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ æ‰«æå­æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
    
    return subfolder_files

def get_output_directory(input_file: str, base_dir: str) -> str:
    """
    ä¸ºè¾“å…¥æ–‡ä»¶ç”Ÿæˆè¾“å‡ºç›®å½•
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        
    Returns:
        è¾“å‡ºç›®å½•è·¯å¾„
    """
    input_path = Path(input_file)
    
    # å¦‚æœå¯ç”¨å­æ–‡ä»¶å¤¹å¤„ç†ï¼Œè¾“å‡ºåˆ°æºæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
    if SUBFOLDER_PROCESSING and INPUT_CONFIG.get("subfolder_processing", {}).get("output_in_source", False):
        # è¾“å‡ºåˆ°æºæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
        return str(input_path.parent)
    else:
        # ä¼ ç»Ÿæ¨¡å¼ï¼šåˆ›å»ºåŸºäºæ–‡ä»¶åçš„è¾“å‡ºç›®å½•
        file_name = input_path.stem
        output_dir = Path(base_dir) / f"{file_name}_processed"
        return str(output_dir)

def process_single_file(input_file: str, output_dir: str, config: PreprocessConfig, 
                       retry_count: int = 0) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        config: é…ç½®å¯¹è±¡
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    try:
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {Path(input_file).name}")
        
        # åˆ›å»ºå¤„ç†ç®¡é“
        pipeline = BatchPreprocessPipeline(config)
        
        # å¤„ç†æ–‡ä»¶
        result = pipeline.run_batch([input_file], output_dir)
        
        print(f"âœ… å®Œæˆæ–‡ä»¶: {Path(input_file).name}")
        return {
            "status": "success",
            "input_file": input_file,
            "output_dir": output_dir,
            "result": result,
            "retry_count": retry_count
        }
        
    except Exception as e:
        error_msg = f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {Path(input_file).name} - {str(e)}"
        print(error_msg)
        
        # é‡è¯•é€»è¾‘
        if retry_count < MAX_RETRIES:
            print(f"ğŸ”„ é‡è¯•å¤„ç† ({retry_count + 1}/{MAX_RETRIES}): {Path(input_file).name}")
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            return process_single_file(input_file, output_dir, config, retry_count + 1)
        
        return {
            "status": "failed",
            "input_file": input_file,
            "output_dir": output_dir,
            "error": str(e),
            "retry_count": retry_count
        }

def process_files_parallel(input_files: List[str], base_output_dir: str, 
                          config: PreprocessConfig) -> List[Dict[str, Any]]:
    """
    å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶
    
    Args:
        input_files: è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        base_output_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        config: é…ç½®å¯¹è±¡
        
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨
    """
    results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤ä»»åŠ¡
        future_to_file = {}
        for input_file in input_files:
            output_dir = get_output_directory(input_file, base_output_dir)
            future = executor.submit(process_single_file, input_file, output_dir, config)
            future_to_file[future] = input_file
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_file):
            input_file = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
                
                if not CONTINUE_ON_ERROR and result["status"] == "failed":
                    print(f"ğŸ›‘ é‡åˆ°é”™è¯¯ï¼Œåœæ­¢å¤„ç†")
                    break
                    
            except Exception as e:
                print(f"âŒ å¤„ç†ä»»åŠ¡æ—¶å‡ºç°æ„å¤–é”™è¯¯: {e}")
                results.append({
                    "status": "failed",
                    "input_file": input_file,
                    "error": str(e)
                })
    
    return results

def process_files_sequential(input_files: List[str], base_output_dir: str, 
                           config: PreprocessConfig) -> List[Dict[str, Any]]:
    """
    é¡ºåºå¤„ç†å¤šä¸ªæ–‡ä»¶
    
    Args:
        input_files: è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        base_output_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        config: é…ç½®å¯¹è±¡
        
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨
    """
    results = []
    
    for i, input_file in enumerate(input_files, 1):
        print(f"\nğŸ“Š è¿›åº¦: {i}/{len(input_files)}")
        
        output_dir = get_output_directory(input_file, base_output_dir)
        result = process_single_file(input_file, output_dir, config)
        results.append(result)
        
        if not CONTINUE_ON_ERROR and result["status"] == "failed":
            print(f"ğŸ›‘ é‡åˆ°é”™è¯¯ï¼Œåœæ­¢å¤„ç†")
            break
    
    return results

def generate_summary_report(results: List[Dict[str, Any]], output_dir: str):
    """
    ç”Ÿæˆæ‰¹é‡å¤„ç†æ€»ç»“æŠ¥å‘Š
    
    Args:
        results: å¤„ç†ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    # ç»Ÿè®¡ä¿¡æ¯
    total_files = len(results)
    successful_files = sum(1 for r in results if r["status"] == "success")
    failed_files = total_files - successful_files
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "batch_processing_summary": {
            "total_files": total_files,
            "successful_files": successful_files,
            "failed_files": failed_files,
            "success_rate": f"{successful_files/total_files*100:.1f}%" if total_files > 0 else "0%"
        },
        "file_results": results,
        "processing_config": config.to_dict(),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(output_dir) / "batch_processing_report.json"
    os.makedirs(output_dir, exist_ok=True)
    
    import json
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“‹ å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡ESGæŠ¥å‘Šé¢„å¤„ç†...")
    
    # ç³»ç»Ÿæ£€æŸ¥
    print("ğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    issues = check_system_requirements()
    if issues:
        print("âš ï¸ ç³»ç»Ÿæ£€æŸ¥å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in issues:
            print(f"   - {issue}")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­
        response = input("æ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿ(y/N): ").lower()
        if response not in ['y', 'yes']:
            print("ğŸ›‘ ç”¨æˆ·é€‰æ‹©é€€å‡º")
            return
    
    # é…ç½®éªŒè¯
    print("\nğŸ” éªŒè¯é…ç½®...")
    errors = validate_config()
    if errors:
        print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"   - {error}")
        print("è¯·ä¿®æ”¹ batch_config.py æ–‡ä»¶ä¸­çš„é…ç½®")
        return
    
    print(f"âœ… é…ç½®éªŒè¯é€šè¿‡ (ç¯å¢ƒ: {ENVIRONMENT})")
    
    # æ”¶é›†è¾“å…¥æ–‡ä»¶
    input_files = []
    
    # æ–¹å¼1: ä»æŒ‡å®šçš„æ–‡ä»¶åˆ—è¡¨
    if INPUT_FILES:
        for file_path in INPUT_FILES:
            if os.path.exists(file_path):
                input_files.append(file_path)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # æ–¹å¼2: å­æ–‡ä»¶å¤¹å¤„ç†æ¨¡å¼
    if SUBFOLDER_PROCESSING:
        subfolder_config = INPUT_CONFIG.get("subfolder_processing", {})
        base_directory = subfolder_config.get("base_directory", INPUT_DIRECTORY)
        exclude_patterns = subfolder_config.get("exclude_folder_patterns", [])
        
        print(f"ğŸ” ä½¿ç”¨å­æ–‡ä»¶å¤¹å¤„ç†æ¨¡å¼ï¼Œæ‰«æç›®å½•: {base_directory}")
        subfolder_files = discover_subfolder_json_files(base_directory, exclude_patterns)
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
        for folder_path, json_files in subfolder_files.items():
            input_files.extend(json_files)
    
    # æ–¹å¼3: ä»ç›®å½•æ‰«æ (ä¼ ç»Ÿæ¨¡å¼)
    elif INPUT_DIRECTORY:
        discovered_files = discover_json_files(INPUT_DIRECTORY, PATTERN)
        input_files.extend(discovered_files)
    
    # å»é‡
    input_files = list(set(input_files))
    
    if not input_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶")
        print("è¯·æ£€æŸ¥ï¼š")
        print("1. INPUT_FILES åˆ—è¡¨ä¸­çš„æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. INPUT_DIRECTORY æ˜¯å¦è®¾ç½®æ­£ç¡®")
        print("3. ç›®å½•ä¸­æ˜¯å¦åŒ…å«.jsonæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(input_files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    
    try:
        if ENABLE_PARALLEL and len(input_files) > 1:
            print(f"ğŸ”„ ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ (å·¥ä½œçº¿ç¨‹: {MAX_WORKERS})")
            results = process_files_parallel(input_files, OUTPUT_BASE_DIR, config)
        else:
            print("ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
            results = process_files_sequential(input_files, OUTPUT_BASE_DIR, config)
        
        # å¤„ç†å®Œæˆ
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        if SUBFOLDER_PROCESSING:
            # å­æ–‡ä»¶å¤¹å¤„ç†æ¨¡å¼ï¼šå°†æŠ¥å‘Šä¿å­˜åœ¨åŸºç¡€ç›®å½•ä¸­
            base_directory = INPUT_CONFIG.get("subfolder_processing", {}).get("base_directory", INPUT_DIRECTORY)
            report = generate_summary_report(results, base_directory)
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šä¿å­˜åœ¨è¾“å‡ºåŸºç¡€ç›®å½•
            report = generate_summary_report(results, OUTPUT_BASE_DIR)
        
        # æ˜¾ç¤ºæ€»ç»“
        summary = report["batch_processing_summary"]
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"   æˆåŠŸæ–‡ä»¶æ•°: {summary['successful_files']}")
        print(f"   å¤±è´¥æ–‡ä»¶æ•°: {summary['failed_files']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„æ–‡ä»¶
        failed_files = [r for r in results if r["status"] == "failed"]
        if failed_files:
            print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶:")
            for failed in failed_files:
                print(f"   - {Path(failed['input_file']).name}: {failed.get('error', 'Unknown error')}")
        
        # æ˜¾ç¤ºè¾“å‡ºä½ç½®
        if SUBFOLDER_PROCESSING:
            print(f"\nğŸ“ è¾“å‡ºä½ç½®: å„æ–‡ä»¶çš„æºæ–‡ä»¶å¤¹ä¸­")
            print(f"   æ¯ä¸ªå­æ–‡ä»¶å¤¹éƒ½åŒ…å«ç›¸åº”çš„å¤„ç†ç»“æœ")
        else:
            print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_BASE_DIR}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å‡ºç°å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main() 