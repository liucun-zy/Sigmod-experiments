#!/usr/bin/env python3
"""
é€šç”¨å…ƒæ•°æ®æ·»åŠ å·¥å…·

ä¸ºESGæŠ¥å‘Šå¤„ç†è¿‡ç¨‹ä¸­çš„JSONæ–‡ä»¶æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- è‚¡ç¥¨ä»£ç 
- å…¬å¸åç§°  
- æŠ¥å‘Šå¹´ä»½
- æŠ¥å‘Šåç§°
- æŠ¥å‘Šç±»å‹

æ”¯æŒæ‰¹é‡å¤„ç†å’Œä¸åŒçš„JSONç»“æ„æ ¼å¼ã€‚
"""

import json
import os
import re
import argparse
from pathlib import Path
from typing import Dict, Any, List, Optional

def extract_metadata_from_filename(file_path: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶è·¯å¾„ä¸­æå–å…ƒæ•°æ®ä¿¡æ¯
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ï¼š
        "/path/300497.SZ-å¯Œç¥¥è¯ä¸š-2024å¹´åº¦ç¯å¢ƒã€ç¤¾ä¼šåŠå…¬å¸æ²»ç†(ESG)æŠ¥å‘Š_grouped.json"
    
    Returns:
        dict: åŒ…å«å…ƒæ•°æ®çš„å­—å…¸
    """
    # è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
    filename = os.path.splitext(os.path.basename(file_path))[0]
    
    # ç§»é™¤å¸¸è§çš„åç¼€
    suffixes_to_remove = ['_grouped', '_vlm', '_clustered', '_aligned', '_processed', '_final']
    for suffix in suffixes_to_remove:
        if filename.endswith(suffix):
            filename = filename[:-len(suffix)]
            break
    
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥è§£ææ–‡ä»¶å
    # æ ¼å¼: è‚¡ç¥¨ä»£ç -å…¬å¸åç§°-æŠ¥å‘Šå¹´ä»½-æŠ¥å‘Šç±»å‹
    pattern = r'^([^-]+)-([^-]+)-(\d{4})å¹´åº¦(.+)$'
    
    match = re.match(pattern, filename)
    
    if match:
        stock_code = match.group(1).strip()
        company_name = match.group(2).strip()
        report_year = int(match.group(3))
        report_type = match.group(4).strip()
        
        # æ„å»ºå®Œæ•´çš„æŠ¥å‘Šåç§°
        report_name = f"{report_year}å¹´åº¦{report_type}"
        
        return {
            "stock_code": stock_code,
            "company_name": company_name,
            "report_year": report_year,
            "report_name": report_name,
            "report_type": report_type,
            "original_filename": os.path.basename(file_path),
            "processing_timestamp": None,  # å¯ä»¥åœ¨å¤„ç†æ—¶æ·»åŠ æ—¶é—´æˆ³
            "file_size_bytes": None,  # å¯ä»¥æ·»åŠ æ–‡ä»¶å¤§å°ä¿¡æ¯
            "data_source": "ESG_REPORT_PROCESSING"  # æ•°æ®æ¥æºæ ‡è¯†
        }
    else:
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é»˜è®¤å€¼
        return {
            "stock_code": "UNKNOWN",
            "company_name": "æœªçŸ¥å…¬å¸",
            "report_year": None,
            "report_name": "æœªçŸ¥æŠ¥å‘Š",
            "report_type": "æœªçŸ¥ç±»å‹",
            "original_filename": os.path.basename(file_path),
            "processing_timestamp": None,
            "file_size_bytes": None,
            "data_source": "ESG_REPORT_PROCESSING"
        }

def add_file_info_to_metadata(metadata: Dict[str, Any], file_path: str) -> Dict[str, Any]:
    """
    ä¸ºå…ƒæ•°æ®æ·»åŠ æ–‡ä»¶ç›¸å…³ä¿¡æ¯
    
    Args:
        metadata: åŸºç¡€å…ƒæ•°æ®å­—å…¸
        file_path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: å¢å¼ºåçš„å…ƒæ•°æ®å­—å…¸
    """
    import time
    
    try:
        file_stat = os.stat(file_path)
        metadata.update({
            "processing_timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
            "file_size_bytes": file_stat.st_size,
            "file_modified_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(file_stat.st_mtime))
        })
    except OSError:
        pass
    
    return metadata

def add_metadata_to_json(input_path: str, output_path: str = None, 
                        structure_type: str = "auto", backup: bool = True) -> Dict[str, Any]:
    """
    ä¸ºJSONæ–‡ä»¶æ·»åŠ å…ƒæ•°æ®
    
    Args:
        input_path: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
        structure_type: JSONç»“æ„ç±»å‹ ("auto", "pages", "blocks", "raw")
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½æ–‡ä»¶
    
    Returns:
        dict: å¤„ç†ç»“æœä¿¡æ¯
    """
    if output_path is None:
        output_path = input_path
    
    # åˆ›å»ºå¤‡ä»½
    if backup and input_path == output_path:
        backup_path = f"{input_path}.backup"
        if os.path.exists(backup_path):
            # å¦‚æœå¤‡ä»½å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
            import time
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            backup_path = f"{input_path}.backup.{timestamp}"
        
        import shutil
        shutil.copy2(input_path, backup_path)
        print(f"ğŸ“ åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path}")
    
    # è¯»å–åŸå§‹JSON
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        return {"status": "error", "message": f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}"}
    
    # æå–å…ƒæ•°æ®
    metadata = extract_metadata_from_filename(input_path)
    metadata = add_file_info_to_metadata(metadata, input_path)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å…ƒæ•°æ®
    if isinstance(data, dict) and "metadata" in data:
        print("âš ï¸  æ–‡ä»¶å·²åŒ…å«å…ƒæ•°æ®ï¼Œå°†è¿›è¡Œæ›´æ–°")
        # æ›´æ–°ç°æœ‰å…ƒæ•°æ®ï¼Œä¿ç•™åŸæœ‰çš„å…¶ä»–å­—æ®µ
        existing_metadata = data["metadata"]
        if isinstance(existing_metadata, dict):
            existing_metadata.update(metadata)
            metadata = existing_metadata
    
    # æ ¹æ®ç»“æ„ç±»å‹å¤„ç†æ•°æ®
    if structure_type == "auto":
        # è‡ªåŠ¨æ£€æµ‹ç»“æ„ç±»å‹
        if isinstance(data, dict):
            if "pages" in data:
                structure_type = "pages"
            elif "metadata" in data:
                structure_type = "existing"
            elif all(key in data for key in ["h1", "h2", "data_type"]) if len(data) > 0 else False:
                structure_type = "blocks"
            else:
                structure_type = "raw"
        elif isinstance(data, list):
            structure_type = "blocks"
        else:
            structure_type = "raw"
    
    # æ„å»ºæœ€ç»ˆè¾“å‡ºç»“æ„
    if structure_type == "pages":
        # å·²ç»æ˜¯pagesç»“æ„ï¼Œåªæ›´æ–°metadata
        final_output = {
            "metadata": metadata,
            "pages": data.get("pages", data)
        }
    elif structure_type == "existing":
        # å·²æœ‰metadataç»“æ„ï¼Œæ›´æ–°metadata
        final_output = data.copy()
        final_output["metadata"] = metadata
    elif structure_type == "blocks":
        # å—ç»“æ„æ•°æ®ï¼ŒåŒ…è£…ä¸ºæ ‡å‡†æ ¼å¼
        final_output = {
            "metadata": metadata,
            "data": data
        }
    else:
        # åŸå§‹æ•°æ®ï¼Œç›´æ¥åŒ…è£…
        final_output = {
            "metadata": metadata,
            "content": data
        }
    
    # ä¿å­˜ç»“æœ
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)
        
        return {
            "status": "success",
            "input_path": input_path,
            "output_path": output_path,
            "structure_type": structure_type,
            "metadata": metadata,
            "backup_created": backup and input_path == output_path
        }
    except Exception as e:
        return {"status": "error", "message": f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}"}

def batch_add_metadata(input_dir: str, pattern: str = "*.json", 
                      output_dir: str = None, backup: bool = True) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡ä¸ºJSONæ–‡ä»¶æ·»åŠ å…ƒæ•°æ®
    
    Args:
        input_dir: è¾“å…¥ç›®å½•
        pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™åŸåœ°æ›´æ–°ï¼‰
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
    
    Returns:
        list: å¤„ç†ç»“æœåˆ—è¡¨
    """
    input_path = Path(input_dir)
    results = []
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
    json_files = list(input_path.glob(pattern))
    
    if not json_files:
        print(f"âŒ åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é… {pattern} çš„æ–‡ä»¶")
        return results
    
    print(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
    
    for json_file in json_files:
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {json_file.name}")
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_dir:
            output_path = Path(output_dir) / json_file.name
            os.makedirs(output_dir, exist_ok=True)
        else:
            output_path = json_file
        
        # æ·»åŠ å…ƒæ•°æ®
        result = add_metadata_to_json(str(json_file), str(output_path), backup=backup)
        result["file_name"] = json_file.name
        results.append(result)
        
        if result["status"] == "success":
            print(f"âœ… æˆåŠŸå¤„ç†: {json_file.name}")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {json_file.name} - {result.get('message', 'Unknown error')}")
    
    return results

def main():
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸ºESGæŠ¥å‘ŠJSONæ–‡ä»¶æ·»åŠ å…ƒæ•°æ®")
    parser.add_argument("input", help="è¾“å…¥JSONæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡ºæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("-t", "--type", choices=["auto", "pages", "blocks", "raw"], 
                       default="auto", help="JSONç»“æ„ç±»å‹")
    parser.add_argument("-p", "--pattern", default="*.json", help="æ‰¹é‡å¤„ç†æ—¶çš„æ–‡ä»¶åŒ¹é…æ¨¡å¼")
    parser.add_argument("--no-backup", action="store_true", help="ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶")
    parser.add_argument("-v", "--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    backup = not args.no_backup
    
    if input_path.is_file():
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        print(f"ğŸš€ å¤„ç†å•ä¸ªæ–‡ä»¶: {input_path}")
        result = add_metadata_to_json(str(input_path), args.output, args.type, backup)
        
        if result["status"] == "success":
            print("âœ… å¤„ç†æˆåŠŸ!")
            if args.verbose:
                print("\nğŸ“Š æå–çš„å…ƒæ•°æ®:")
                for key, value in result["metadata"].items():
                    print(f"   {key}: {value}")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result.get('message', 'Unknown error')}")
    
    elif input_path.is_dir():
        # æ‰¹é‡å¤„ç†ç›®å½•
        print(f"ğŸš€ æ‰¹é‡å¤„ç†ç›®å½•: {input_path}")
        results = batch_add_metadata(str(input_path), args.pattern, args.output, backup)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r["status"] == "success")
        total_count = len(results)
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ:")
        print(f"   æ€»æ–‡ä»¶æ•°: {total_count}")
        print(f"   æˆåŠŸå¤„ç†: {success_count}")
        print(f"   å¤±è´¥å¤„ç†: {total_count - success_count}")
        
        if args.verbose and results:
            print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
            for result in results:
                status_icon = "âœ…" if result["status"] == "success" else "âŒ"
                print(f"   {status_icon} {result['file_name']}")
    
    else:
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {input_path}")

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œæ‰§è¡Œå‘½ä»¤è¡Œæ¥å£
    main()

# ä½¿ç”¨ç¤ºä¾‹:
"""
# 1. ä¸ºå•ä¸ªæ–‡ä»¶æ·»åŠ å…ƒæ•°æ®
python add_metadata.py report.json

# 2. ä¸ºå•ä¸ªæ–‡ä»¶æ·»åŠ å…ƒæ•°æ®å¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶
python add_metadata.py report.json -o report_with_metadata.json

# 3. æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
python add_metadata.py /path/to/reports/

# 4. æ‰¹é‡å¤„ç†ç‰¹å®šæ¨¡å¼çš„æ–‡ä»¶
python add_metadata.py /path/to/reports/ -p "*_grouped.json"

# 5. å¤„ç†æ—¶ä¸åˆ›å»ºå¤‡ä»½
python add_metadata.py report.json --no-backup

# 6. è¯¦ç»†è¾“å‡ºæ¨¡å¼
python add_metadata.py report.json -v
""" 